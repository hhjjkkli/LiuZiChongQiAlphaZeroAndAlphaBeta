2019-03-19
之前去掉重复代码后代码量少了近200行,现在添加了把所有谱(训练的,评测的,对弈的)都放到数据库,从数据库读取数据打谱,从数据库直接获取数据进行训练几个功能,直接用sql就可以统计出各种胜率了,现在代码量是1500多.
昨晚本机运行6小时40分只生成了100谱,生成谱是纯Python代码在生成,运行效率很低,但是从数据库读取谱出来后直接训练有GPU加速就很快.网络有规律之后成谱速度会快很多,才开始时乱走还出现过一个谱花了30分钟才走完.
功能大概已经够了.代码也差不多理清,用mcts的探索来修正神经网络,这个想法真的很奇妙.

black平面记录黑子的位置,有子为1,无子为0;
white平面记录白子的位置,有子为1,无子为0;
rival_last_move_position平面记录对手最后一次走子后子的位置(该走动会造成吃子,所以猜测它可能对训练很重要);
is_current_black平面记录当前行棋方是不是先手,也即是不是黑棋,为先手时全为1,否则全为0;

2019-03-20
前面的AlphaZero_0到AlphaZero_239是在win8本机上跑的谱.
cd /usr/local/python3.6/bin
./pip3 install -r /home/LiuZiChongQiFinal/requirements.txt
发现要去掉其中的PyOpenGL包,因为在centos环境下会找不到依赖.其实这个包之前在win上也是单独安装的,不是用pip安装的.
之前好像是一周跑了2000谱,而且是加了100回合即盘和的限制下得到的.现在还不确定需要多少时间.
centos安装依赖完成:
Installing collected packages: six, absl-py, astor, html5lib, bleach, Pillow, captcha, certifi, pycparser, cffi, chardet, colorama, cssselect, cycler, Cython, gast, grpcio, numpy, h5py, idna, PyYAML, scipy, Keras, kiwisolver, lxml, Markdown, pytz, python-dateutil, pyparsing, matplotlib, mpmath, pandas, prettytable, protobuf, pygame, PyMySQL, python-xlib, pynput, pyquery, urllib3, requests, scikit-learn, selenium, sklearn, splinter, sympy, wheel, tensorboard, termcolor, tensorflow, tensorflow-tensorboard, tqdm
  Running setup.py install for absl-py ... done
  Running setup.py install for html5lib ... done
  Running setup.py install for captcha ... done
  Running setup.py install for pycparser ... done
  Running setup.py install for gast ... done
  Running setup.py install for PyYAML ... done
  Running setup.py install for mpmath ... done
  Running setup.py install for prettytable ... done
  Running setup.py install for sklearn ... done
  Running setup.py install for splinter ... done
  Running setup.py install for sympy ... done
  Running setup.py install for termcolor ... done
Successfully installed Cython-0.28.3 Keras-2.0.8 Markdown-2.6.11 Pillow-5.2.0 PyMySQL-0.9.3 PyYAML-3.12 absl-py-0.4.1 astor-0.7.1 bleach-1.5.0 captcha-0.2.4 certifi-2018.4.16 cffi-1.11.5 chardet-3.0.4 colorama-0.4.1 cssselect-1.0.3 cycler-0.10.0 gast-0.2.0 grpcio-1.14.2 h5py-2.7.1 html5lib-0.9999999 idna-2.6 kiwisolver-1.0.1 lxml-4.2.4 matplotlib-2.2.2 mpmath-1.0.0 numpy-1.14.5 pandas-0.22.0 prettytable-0.7.2 protobuf-3.6.1 pycparser-2.19 pygame-1.9.3 pynput-1.4 pyparsing-2.2.0 pyquery-1.4.0 python-dateutil-2.7.2 python-xlib-0.25 pytz-2018.4 requests-2.18.4 scikit-learn-0.19.2 scipy-1.0.1 selenium-3.141.0 six-1.11.0 sklearn-0.0 splinter-0.10.0 sympy-1.1.1 tensorboard-1.10.0 tensorflow-1.10.0 tensorflow-tensorboard-0.1.8 termcolor-1.1.0 tqdm-4.23.2 urllib3-1.22 wheel-0.33.1

loss, entropy = self.policyValueNet.doOneTrain(batchState, batchProbability, batchScore,
                                                           self.learningRate * self.learningRateMultiplier)
每次重启TrainPipline时,learningRateMultiplier值都会被重设为1,不过这个应该影响不大.
参考如下资料:
https://blog.csdn.net/weixin_42840933/article/details/85780125
使用如下命令让训练流水线在后台运行:
nohup python3 TrainPipeline.py >> /home/liuzichongqi.log 2>&1 &
nohup python3 TrainPipeline.py >> /dev/null 2>&1 &
用jobs -l查看后台任务

2019-03-21
发现之前这里的一个问题
gameDatas = Util.readGameFromDB(readAll=True, type='train')
之前这里没有指定type,那么评测的谱也被吃掉了而且还写到current_policy文件里去了,这些谱肯定是不正常的,不过也只有20个谱,应该影响不大.
记不清了,可能也没有吃掉这20个谱,因为之后都是直接从文件取现有权重没有从数据库从头训练.不过,即使吃掉了应该也影响不大,更何况可能没有吃掉,
主要是现在已经有500谱了,不忍心重来.

ps -e | grep python
kill 进程号
git add -A

win8上的Python版本是3.6.3, centos上的是3.6.2

评测间隔改为1000,因为这个太费时间,而且也不影响生成的权重.

2019-03-22
了解如何可视化搜索树.

2019-03-23
AlphaZero_100 vs PureMCTS_500 evaluation, AlphaZero win ratio: 0.5
AlphaZero_200 vs PureMCTS_500 evaluation, AlphaZero win ratio: 0.8
AlphaZero_300 vs PureMCTS_500 evaluation, AlphaZero win ratio: 1.0
AlphaZero_400 vs PureMCTS_1000 evaluation, AlphaZero win ratio: 0.9
AlphaZero_500 vs PureMCTS_1000 evaluation, AlphaZero win ratio: 0.5
AlphaZero_1000 vs PureMCTS_1000 evaluation, AlphaZero win ratio: 0.3
从400谱到1000谱,为什么对纯mcts的胜率反而在下降?
之前的规则是: 允许循环走子,但是超过100回合没结束就判和.在此规则下训练了2000谱,然后和ab算法下会开局就循环.
后来把规则改为不允许出现任何重复局面,用在老规则下生成的模型和同样的ab算法下,执黑会负,但是执白会把黑方六子困毙而胜,
分析原因发现ab算法的估值函数没有给可走动方式给足够的分数,改了分数之后不会再被困毙,但是在268回合后,AlphaZero仍然执白取胜.
提供给网络的数据为:
1.状态:
表示对局状态的四个平面,每个平面都是4*4大小,所以尺寸为(状态个数,4,4,4):
平面1记录黑子的位置,有子为1,无子为0;
平面2记录白子的位置,有子为1,无子为0;
平面3记录对手最后一次走子后子的位置(该走动会造成吃子,所以猜测它可能对训练很重要);
平面4记录当前行棋方是不是先手,也即是不是黑棋,为先手时全为1,否则全为0.
2.拟合数据:
2.1 mcts探索出的概率,尺寸为(状态个数,64)
2.2 自对弈胜负结果分数,以1,0,-1标识胜,平,负,尺寸为(状态个数,1)
这些数据在没限制不允许重复局面出现时,似乎是合理的.
但是现在有了不允许出现历史局面的限制,却没有告诉网络历史局面有哪些,这是不是不合理,或者是胜率下降的原因?
比如: A,B为两个相同的状态,但是要拟合的数据会有区别,如果A可以选择所有可能的6种走子方式,但是B却因为其中一种会导致历史局面出现而只能选择5种,
这样的话,网络自己是不是蒙了?明明是相同的状态,怎么要拟合不一样的数据呢?如果之前拟合了A,其中A独有的那一个走子方式的概率会在拟合B时下降,但是
即便下降了,却也会因为有过A的出现而比其它走子方式的概率高,但是,如果说就是这个走子方式才是最好的,现在却因为B使得它变得不如其它走子方式的概率高了,
导致网络和别人对弈时在6种方式都可以选择的情况下选择了较差的走子方式,这就不正确了.
如果要改,应该改输入网络的状态数据,直接给一个(1,64)尺寸的数据表示64种走子方式中哪些能走哪些不能走可以吗?能走的就置1,不能的置0,这就区分了
有无历史局面出现时的状态区别.
我要不要改呢?我之前参考的是那个五子棋项目,但是五子棋是不会出现重复局面的,8*8棋盘也就64点最多64回合也就放满了,但是这个棋目前出现的最大回合数达到了395,
之前是限制了100的上限还好点,现在的规则在2子对1子局面也没有直接判胜,而是要走到最后,就这种残局极度浪费回合数.

不限制重复局面对网络来说也是合理的,毕竟重复会导致超过100回合而被判和得0分,而判胜会得1分,所以它在能取胜时是不会选择循环得0分的啊?但是之前也没有
把回合数这个数据输入给网络,而且即使输入给它,它也只知道个回合数,真的有用吗?它又不会像ab算法那样预测对手走法,怎么知道会不会循环?

但是即便增加(1,64)尺寸的数据表示64种走子方式中哪些能走哪些不能走输入给网络,网络也还是不知道当前历史局面到底已经有了哪些局面,它的预测还是没有
考虑到这个必须要考虑的数据,但是历史局面数据怎么编码呢?这个是真的太复杂了.

站在ab算法的角度上看,预测对手走法是必须的,可是神经网络仅仅是根据局面输出概率,预测对手走法的任务给了mcts.对啊,还有mcts,
而mcts在探索时是知道历史局面有哪些的,因为它会受到历史局面限制,不能选择让历史局面复现的action,那么mcts是否会在探索后发现
那个网络认为概率低于其它走子方式而实际上为最优的那个action是最佳的呢?不得而知.

目前已经有1054谱了,改网络结构重来有些不太愿意,已经跑了5天谱了,虽然反正是放在腾讯云的1元的学生机1GHZ的CPU上跑的不用心疼自己电脑.

ab算法无疑是绝对正确的,它的问题只是在于全搜索需要的计算力是不可能做到的,而估值函数则是委屈求全的产物,估值函数不可能完美,
而现在AlphaZero的mcts因为充满了随机性也不可能完美,向神经网络输入的state也不一定面面俱到就比如这里就难以提供历史局面的信息,神经网络
是个庞大而又无法理解的估值函数,这个估值函数也不可能完美.
即便是在这4*4的棋盘上,也有3^16种可能的状态,之前试过用纯的强化学习来统计胜负概率,才意识到3^16也是个极大的数据无法统计,纯强化学习也就玩玩
3*3井字棋就够了.
棋的解必须推演到最后一步才能知道,这个事实,无论如何也违背不了.所以,就这样懒得改吧,本来也就不该妄想能够找到最优解.明知网络结构不合理,也凑合着
用吧,就像ab算法明知估值函数不合理,也只能凑合着用一样的.
先看看到2000谱时的胜率是怎样的再说吧,两天之后就知道了,现在改为1000谱一测了,因为评测很浪费时间.
从评测的最后一谱来看,不合理的网络还是有棋力的.

修改棋规为: 重复局面一出现即判定双方和棋.
这样似乎就可以了,和棋分数是0分,低于胜利的1分,网络配合mcts会知道和棋的存在,就会倾向于选择导致胜的走子方式.这和不允许和棋出现时只能靠mcts
知道来感知历史局面的存在不是一回事吗?
所以,把这个任务交给mcts吧.

2019-03-24
我在中国象棋Zero的QQ群里问了作者,他说应该把历史局面加到state里.所以我看了下他的源代码.
超过200回合直接判和,他竟然也这样做,难道我之前限制100回合的做法还更好吗?

我发现并解决了AlphaBeta中的一个bug:把对手的可移动方式个数分算到了自己的头上.然后AlphaBeta在执白和执黑时在新规则下都战胜了老模型.
我在中国象棋Zero的QQ群里问了作者,他说应该把历史局面加到state里.可是他那个项目代码量近7000,很乱,没一点注释,我也不想单步调试来看他
是如何把历史数据加入到网络中去的.但是不解决网络的问题,似乎AlphaZero就不可能战胜AlphaBeta了.

2019-04-18
尝试把AlphaBeta算法每一层选择的move记录下来,不过这有点麻烦,除了第0层,其它层都会有多个结点,每个结点有各自不同的最优move,这需要构造一个大的
dict,感觉很浪费性能.
到5000谱之后程序退出,可是对mcts_1500的胜率只有0.9,所以调到10000后继续跑谱,可是学习率参数是动态的,导致胜率在6000时下降到了0.6.
对局数: 7778
占用磁盘: 770.64MB
耗时: 一个月
到9月还有5个月,则对局上限设为: 200,000, 即使达到也只是20GB, 够用.
中断,重新启动训练流程.当前对局数为7779.

2019-04-23
在网络中加入graph后,报错:
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key optimizer/beta1_power not found in checkpoint
但是发现canloop目录下没有checkpoint文件也照样可以加载模型,所以试一下直接删除此文件.
然而还是报错
NotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

Key optimizer/beta1_power not found in checkpoint
无奈,只好重新吃谱.
在云主机运行吃谱,使用如下命令查看数据变化:
tensorboard --logdir from_db_train_log
输出的是本地地址:
http://VM_5_251_centos:6006
试一下以公网地址访问:
http://111.230.145.180:6006/
果然可以.
用如下命令在后台运行,就可以随时观察训练过程了:
nohup tensorboard --logdir from_db_train_log >> /dev/null 2>&1 &
不小心执行了下面这个语句,误删了吃谱记录,恢复需要学习MySQL日志,太麻烦
delete from policy_update where type='from_db'; # 误删了
只好重新开始吃谱

